---
title: "Introduction"
author: "Wang Xiaoli"
date: "2023-12-04"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Outlook

__SA23204175__ is developed to replicate the first three methods of scaled sparse linear regression by Tingni Sun and Cunhui Zhang (2012). These three methods are the l1 penalized maximum likelihood estimator, the bias correction estimator and the scaled lasso estimator, respectively.

Moreover, the existing R package 'scalreg' by Tingni Sun is used to compute the scaled lasso estimator. As for the PMLE estimator and the BC estimator, the R functions _PMLE.Iter.R_ and _BC.APMLE.R_ have been written to compute the estimators. 

Our evaluation index uses $\hat{\sigma}/\sigma$ and $\bar{\sigma}/\sigma$. The Rcpp function _compute_ratio_ has been written to compute $\hat{\sigma}/\sigma$ and even $\bar{\sigma}/\sigma$.

## Simple Introduction Of Estimation Methods

### The L1 Penalized Maximum Likelihood Estimator

The l1 penalized maximum likelihood estimator (PMLE) is
$$(\hat{\beta}^{(\mathrm{pmle})},\hat{\sigma}^{(\mathrm{pmle})})=\mathrm{argmax}_{\beta,\sigma}(-\frac{|y-X\beta|_2^2}{2n\sigma^2}-\log\sigma-\lambda_0\frac{|\beta|_1}{\sigma}).$$

Moreover, the objective function is globally convex and the PMLE estimator is equivalent to the limit of the iteration $\hat{\sigma}\leftarrow\{y'(y-X\hat{\beta})/n\}^{1/2}$ and $\hat{\beta}\leftarrow\hat{\beta}(\hat{\sigma}\lambda_0)$ where $\hat{\beta}(\lambda)$ is the lasso estimator with the penalty level $\lambda$: $\hat{\beta}(\lambda)=\mathrm{argmin}_{\beta}\frac{|y-X\beta|_2^2}{2n}+\lambda|\beta|_1$.

### The Bias Correction Estimator

The bias correction estimator (BC) is one iteration with $(\hat{\beta}^{(\mathrm{pmle})},\hat{\sigma}^{(\mathrm{pmle})})$:
$$\hat{\sigma}^{(\mathrm{bc})}=|y-X\hat{\beta}(\hat{\sigma}^{(\mathrm{pmle})}\lambda_0)|/n^{1/2},\hat{\beta}^{(\mathrm{bc})}=\hat{\beta}(\hat{\sigma}^{(\mathrm{bc})}\lambda_0).$$

The BC estimator is used to remove the positive bias of the noise level in the PMLE estimator.

### The Scaled Lasso Estimator

The scaled lasso estimator has the form:
$$(\hat{\beta}^{(\mathrm{SL})},\hat{\sigma}^{(\mathrm{SL})})=\mathrm{argmin}_{\beta,\sigma}\frac{|y-X\beta|_2^2}{2n\sigma}+\frac{\sigma}{2}+\lambda_0|\beta|_1.$$

Moreover, the scaled lasso estimator is equivalent to the limit of the iteration $\hat{\sigma}\leftarrow|y-X\hat{\beta}|_2/n^{1/2}$ and $\hat{\beta}\leftarrow\hat{\beta}(\hat{\sigma}\lambda_0)$.

Tingni Sun and Cunhui Zhang(2012) is mainly based on their early work "comments on: l1-penalization for mixture regression models by Stadler et al.". This comment paper demonstrates with simulation the superiority of the scaled lasso estimator on removing the positive bias.

## Simulation Study

### Source R code for _PMLE.Iter.R_ and _BC.APMLE.R_

The source R code for _PMLE.Iter.R_ is displayed below.

```{r,eval=FALSE}
PMLE.Iter.R<-function(x,y,lambda0,max.it=5000){
  n<-nrow(x);p<-ncol(x)
  beta.f<-rep(0,p)
  beta<-beta.f;sigma<-0.1
  for(i in 1:max.it){
    sigma.hat<-sqrt(sum(y*(y-x%*%beta))/n)
    fit.hat<-lars(x,y,type="lasso",intercept=F,normalize=F,use.Gram=F)
    beta.hat<-predict.lars(fit.hat,s=n*lambda0*sigma.hat,type="coefficients",mode="lambda")$coef
    if(abs(sigma-sigma.hat)<1e-6) break
    beta<-beta.hat;sigma<-sigma.hat
  }  
  list(beta.pmle=beta.hat,sigma.pmle=sigma.hat)
}
```

The source R code for _BC.APMLE.R_ is displayed below.

```{r,eval=FALSE}
BC.APMLE.R<-function(x,y,lambda0,beta.pmle,sigma.pmle){
  n<-nrow(x);p<-ncol(x)
  f<-lars(x,y,type="lasso",intercept=F,normalize=F,use.Gram=F)
  y.hat<-predict.lars(f,x,s=n*lambda0*sigma.pmle,type="fit",mode="lambda")$fit
  sigma.bc<-sqrt(mean((y-y.hat)^2))
  beta.bc<-predict.lars(f,s=n*lambda0*sigma.bc,type="coefficients",mode="lambda")$coef
  list(beta.bc=beta.bc,sigma.bc=sigma.bc) 
}
```

### Simulation Of Partial Results In Table 1

```{r}
library(SA23204175)
library(scalreg)
library(lars)
```

```{r}
#Initialize parameters
n<-200;p<-2000;r0<-0.5
lambda1<-sqrt(log(p)/n)
lambda2<-sqrt(2*log(p)/n)
lambda3<-sqrt(4*log(p)/n)

#Generate covariance matrix Sigma of Gaussian distribution
#library(pracma) #create basic matrices like eye(n,m=n), ones(n,m=n) and zeros(n,m=n)
Sigma1<-pracma::eye(p);Sigma2<-0.5*pracma::eye(p)+0.5*pracma::ones(p) 

#Generate design matrix X
library(MASS);set.seed(123)
X1<-mvrnorm(n,rep(0,p),Sigma1)
X2<-mvrnorm(n,rep(0,p),Sigma2)

#Initialize parameters
sigma<-1;beta.star<-matrix(0,nrow=p,ncol=1);beta.star[1,1]<-1/sqrt(3)
beta.star[2,1]<-1/sqrt(3);beta.star[3,1]<-1/sqrt(3)

#Generate response vector y
set.seed(123)
y1<-as.vector(X1%*%beta.star)+rnorm(n,0,sigma^2)
y2<-as.vector(X2%*%beta.star)+rnorm(n,0,sigma^2)
```

```{r}
#Method-PMLE Dataset-(X1,y1) LSE-FALSE

PMLE.LSEF.1<-list(l1=PMLE.Iter.R(X1,y1,lambda0=lambda1),l2=PMLE.Iter.R(X1,y1,lambda0=lambda2),l3=PMLE.Iter.R(X1,y1,lambda0=lambda3))

#Method-BC Dataset-(X1,y1) LSE-FALSE 

BC.LSEF.1<-list(l1=BC.APMLE.R(X1,y1,lambda0=lambda1,PMLE.LSEF$l1$beta.pmle,PMLE.LSEF.1$l1$sigma.pmle),
                l2=BC.APMLE.R(X1,y1,lambda0=lambda2,PMLE.LSEF$l2$beta.pmle,PMLE.LSEF.1$l2$sigma.pmle),
                l3=BC.APMLE.R(X1,y1,lambda0=lambda3,PMLE.LSEF$l3$beta.pmle,PMLE.LSEF.1$l3$sigma.pmle))

#Method-SLASSO Dataset-(X1,y1) LSE-FALSE 

SL.LSEF.1<-list(l1=scalreg(X1,y1,lam0=lambda1,LSE=FALSE),
                l2=scalreg(X1,y1,lam0=lambda2,LSE=FALSE),
                l3=scalreg(X1,y1,lam0=lambda3,LSE=FALSE))

#Method-PMLE Dataset-(X2,y2) LSE-FALSE

PMLE.LSEF.2<-list(l1=PMLE.Iter.R(X2,y2,lambda0=lambda1),l2=PMLE.Iter.R(X2,y2,lambda0=lambda2),l3=PMLE.Iter.R(X2,y2,lambda0=lambda3))

#Method-BC Dataset-(X2,y2) LSE-FALSE 

BC.LSEF.2<-list(l1=BC.APMLE.R(X2,y2,lambda0=lambda1,PMLE.LSEF$l1$beta.pmle,PMLE.LSEF.2$l1$sigma.pmle),
                l2=BC.APMLE.R(X2,y2,lambda0=lambda2,PMLE.LSEF$l2$beta.pmle,PMLE.LSEF.2$l2$sigma.pmle),
                l3=BC.APMLE.R(X2,y2,lambda0=lambda3,PMLE.LSEF$l3$beta.pmle,PMLE.LSEF.2$l3$sigma.pmle))

#Method-SLASSO Dataset-(X2,y2) LSE-FALSE 

SL.LSEF.2<-list(l1=scalreg(X2,y2,lam0=lambda1,LSE=FALSE),
                l2=scalreg(X2,y2,lam0=lambda2,LSE=FALSE),
                l3=scalreg(X2,y2,lam0=lambda3,LSE=FALSE))
```

```{r}
result1<-matrix(0,nrow=9,ncol=2)
rownames(result1)<-c("PMLE/Lambda1","PMLE/Lambda2","PMLE/Lambda3",
                     "BC/Lambda1","BC/Lambda2","BC/Lambda3",
                     "SLASSO/Lambda1","SLASSO/Lambda2","SLASSO/Lambda3")
colnames(result1)<-c("sigma.hat/sigma*","AMS")
result1[1,1]<-PMLE.LSEF.1$l1$sigma.pmle
result1[2,1]<-PMLE.LSEF.1$l2$sigma.pmle
result1[3,1]<-PMLE.LSEF.1$l3$sigma.pmle
result1[4,1]<-BC.LSEF.1$l1$sigma.bc
result1[5,1]<-BC.LSEF.1$l2$sigma.bc
result1[6,1]<-BC.LSEF.1$l3$sigma.bc
result1[7,1]<-SL.LSEF.1$l1$hsigma
result1[8,1]<-SL.LSEF.1$l2$hsigma
result1[9,1]<-SL.LSEF.1$l3$hsigma

result1[1,2]<-sum(PMLE.LSEF.1$l1$beta.pmle!=0)
result1[2,2]<-sum(PMLE.LSEF.1$l2$beta.pmle!=0)
result1[3,2]<-sum(PMLE.LSEF.1$l3$beta.pmle!=0)
result1[4,2]<-sum(BC.LSEF.1$l1$beta.bc!=0)
result1[5,2]<-sum(BC.LSEF.1$l2$beta.bc!=0)
result1[6,2]<-sum(BC.LSEF.1$l3$beta.bc!=0)
result1[7,2]<-sum(SL.LSEF.1$l1$coefficients!=0)
result1[8,2]<-sum(SL.LSEF.1$l2$coefficients!=0)
result1[9,2]<-sum(SL.LSEF.1$l3$coefficients!=0)

result2<-matrix(0,nrow=9,ncol=2)
rownames(result2)<-c("PMLE/Lambda1","PMLE/Lambda2","PMLE/Lambda3",
                     "BC/Lambda1","BC/Lambda2","BC/Lambda3",
                     "SLASSO/Lambda1","SLASSO/Lambda2","SLASSO/Lambda3")
colnames(result2)<-c("sigma.hat/sigma*","AMS")
result2[1,1]<-PMLE.LSEF.2$l1$sigma.pmle
result2[2,1]<-PMLE.LSEF.2$l2$sigma.pmle
result2[3,1]<-PMLE.LSEF.2$l3$sigma.pmle
result2[4,1]<-BC.LSEF.2$l1$sigma.bc
result2[5,1]<-BC.LSEF.2$l2$sigma.bc
result2[6,1]<-BC.LSEF.2$l3$sigma.bc
result2[7,1]<-SL.LSEF.2$l1$hsigma
result2[8,1]<-SL.LSEF.2$l2$hsigma
result2[9,1]<-SL.LSEF.2$l3$hsigma

result2[1,2]<-sum(PMLE.LSEF.2$l1$beta.pmle!=0)
result2[2,2]<-sum(PMLE.LSEF.2$l2$beta.pmle!=0)
result2[3,2]<-sum(PMLE.LSEF.2$l3$beta.pmle!=0)
result2[4,2]<-sum(BC.LSEF.2$l1$beta.bc!=0)
result2[5,2]<-sum(BC.LSEF.2$l2$beta.bc!=0)
result2[6,2]<-sum(BC.LSEF.2$l3$beta.bc!=0)
result2[7,2]<-sum(SL.LSEF.2$l1$coefficients!=0)
result2[8,2]<-sum(SL.LSEF.2$l2$coefficients!=0)
result2[9,2]<-sum(SL.LSEF.2$l3$coefficients!=0)
```

```{r}
cat("result1 with (X1,y1)(r0=0)\n");result1
cat("\nresult2 with (X2,y2)(r0=0.5)\n");result2
```

Give simple explanations about this simple simulation with only one replication (100 replications in the paper 'scaled sparse linear regression';see more details in our complete simulation document [if necessary, contact me with email]). 

1.Both bias correction (BC) and scaled lasso (SLASSO) could remove some positive bias of PMLE. Moreover, scaled lasso performs better than bias correction on it. (Notation: see scaled lasso in result1 and find these abnormal values, here regard it as an accident because in our complete simulation study the results are consistent with the paper 'scaled sparse linear regression'.)

2.Lambda2 seems to be the best choice among lambda1, lambda2 and lambda3.

3.Average model sizes (AMS) of result1 is close to the true model size 3. But average model sizes of result2 is too large. Here give an explanation to it: due to the setting of r0=0.5, rows of X2 are related resulting in large average model sizes. 
